{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhpieX_H51FU",
        "outputId": "1b01400e-f2a6-4f6b-e024-c2f34698fd3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m0.9/1.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wXVadOkb8jI4"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    # Model Configuration\n",
        "    \"vocab_size\": 50257,           # Size of vocabulary (tokenized)\n",
        "    \"context_length\": 512,        # Maximum sequence length is 1024 (context window)\n",
        "    \"emb_dim\": 768,                # Embedding dimension (vector size for each token)\n",
        "    \"n_heads\": 12,                 # Number of attention heads in the MultiHeadAttention\n",
        "    \"n_layers\": 12,                # Number of Transformer layers\n",
        "    \"drop_rate\": 0.1,              # Dropout rate for regularization\n",
        "    \"qkv_bias\": False,             # Whether to add bias to the Q, K, V linear projections\n",
        "\n",
        "    # Training Configuration\n",
        "    \"batch_size\":4 ,               # Batch size for training (adjust as needed)\n",
        "    \"learning_rate\": 1e-4,         # Learning rate for the optimizer\n",
        "    \"weight_decay\": 0.001,          # Weight decay for regularization\n",
        "    \"epochs\": 25              # Number of training epochs\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0ZROegrl789S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, text_file=None, csv_file=None, tokenizer=None, max_length=256, stride=128):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if csv_file:\n",
        "            # Load CSV data\n",
        "            data = pd.read_csv(csv_file)\n",
        "\n",
        "            # Ensure the required columns are present\n",
        "            if not {\"focus_area\", \"question\", \"answer\"}.issubset(data.columns):\n",
        "                raise ValueError(\"CSV file must contain 'focus_area', 'question', and 'answer' columns\")\n",
        "\n",
        "            # Format focus area, question, and answer pairs\n",
        "            formatted_data = [\n",
        "                f\"Focus Area: {row['focus_area']}\\nQuestion: {row['question']}\\nAnswer: {row['answer']}\\n<|endoftext|>\"\n",
        "                for _, row in data.iterrows()\n",
        "            ]\n",
        "\n",
        "            # Tokenize and create chunks\n",
        "            for text in formatted_data:\n",
        "                tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "                self._create_sequences(tokens, max_length, stride)\n",
        "\n",
        "        elif text_file:\n",
        "            # Load plain text file (existing functionality)\n",
        "            with open(text_file, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "            tokens = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "            self._create_sequences(tokens, max_length, stride)\n",
        "\n",
        "    def _create_sequences(self, tokens, max_length, stride):\n",
        "        for i in range(0, len(tokens) - max_length, stride):\n",
        "            input_chunk = tokens[i:i + max_length]\n",
        "            target_chunk = tokens[i + 1:i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Bn7bsx_97vfR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import tiktoken\n",
        "#from gpt_dataset_class import GPTDataset  # Import the GPTDataset class\n",
        "\n",
        "def load_and_split_data(text_file=None, csv_file=None, config=None):\n",
        "    if text_file is None and csv_file is None:\n",
        "        raise ValueError(\"Either 'text_file' or 'csv_file' must be provided.\")\n",
        "    if text_file is not None and csv_file is not None:\n",
        "        raise ValueError(\"Provide only one of 'text_file' or 'csv_file'. Not both.\")\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")  # Using GPT-2 tokenizer\n",
        "\n",
        "    # Load the dataset using GPTDataset\n",
        "    dataset = GPTDataset(\n",
        "        text_file=text_file,\n",
        "        csv_file=csv_file,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=config[\"context_length\"],  # Max length for tokenized input\n",
        "        stride=config[\"context_length\"] // 2  # Overlap for sliding window\n",
        "    )\n",
        "\n",
        "    total_size = len(dataset)  # Length of the dataset\n",
        "    train_ratio = 0.9  # 90% for training and 10% for validation\n",
        "    train_size = int(train_ratio * total_size)\n",
        "    val_size = total_size - train_size\n",
        "\n",
        "    # Sanity checks to ensure enough data for training and validation\n",
        "    total_tokens = len(dataset.input_ids) * config[\"context_length\"]\n",
        "    if total_tokens * train_ratio < config[\"context_length\"]:\n",
        "        raise ValueError(\n",
        "            \"Not enough tokens for the training loader. \"\n",
        "            \"Try to lower the `context_length` or increase the `training_ratio`.\"\n",
        "        )\n",
        "\n",
        "    if total_tokens * (1 - train_ratio) < config[\"context_length\"]:\n",
        "        raise ValueError(\n",
        "            \"Not enough tokens for the validation loader. \"\n",
        "            \"Try to lower the `context_length` or decrease the `training_ratio`.\"\n",
        "        )\n",
        "\n",
        "    # Split dataset into train and validation sets\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create DataLoader for train and validation datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GJsOjHMC8pkn"
      },
      "outputs": [],
      "source": [
        "# GELU class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5B9Io5np8tAX"
      },
      "outputs": [],
      "source": [
        "# FeedForward class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from src.models.gelu import GELU  # Assuming GELU is a custom activation function\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, emb_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the FeedForward network using nn.Sequential\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 4 * emb_dim),  # First Linear layer\n",
        "            GELU(),                           # Activation function\n",
        "            nn.Linear(4 * emb_dim, emb_dim),  # Second Linear layer\n",
        "            nn.Dropout(dropout)               # Dropout for regularization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)  # Forward pass through the sequential layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KZzqcYDp8zKY"
      },
      "outputs": [],
      "source": [
        "# LayerNorm class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "#builin class\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# class LayerNorm(nn.Module):\n",
        "#     def __init__(self, emb_dim, eps=1e-5):\n",
        "#         super().__init__()\n",
        "#         self.norm = nn.LayerNorm(emb_dim, eps=eps)  # Use built-in LayerNorm\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.norm(x)  # Forward pass through built-in LayerNorm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7tgUOsnB85xQ"
      },
      "outputs": [],
      "source": [
        "# MultiHeadAttention class\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"Embedding dimension(d_out) must be divisible by number of heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Create the causal mask for self-attention\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        # Assert that num_tokens is less than or equal to context_length\n",
        "        assert num_tokens <= self.mask.shape[0], \"Sequence length exceeds context length\"\n",
        "\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        queries = self.W_query(x)\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # Reshape into multi-head dimensions\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = queries @ keys.transpose(-2, -1)\n",
        "\n",
        "        # Apply the causal mask\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "        attn_scores.masked_fill_(mask_bool, float('-inf'))\n",
        "\n",
        "        # Compute attention weights and apply dropout\n",
        "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Compute context vectors\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "        # Apply the final projection\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "82XumrRR88_3"
      },
      "outputs": [],
      "source": [
        "# TransformerBlock class\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from src.models.multi_head_attention import MultiHeadAttention\n",
        "# from src.models.feed_forward import FeedForward\n",
        "# from src.models.layer_norm import LayerNorm\n",
        "# from src.models.config import GPT_CONFIG_124M\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Access configuration from GPT_CONFIG_124M\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=GPT_CONFIG_124M[\"emb_dim\"],\n",
        "            d_out=GPT_CONFIG_124M[\"emb_dim\"],\n",
        "            context_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "            num_heads=GPT_CONFIG_124M[\"n_heads\"],\n",
        "            dropout=GPT_CONFIG_124M[\"drop_rate\"],\n",
        "            qkv_bias=GPT_CONFIG_124M[\"qkv_bias\"]\n",
        "        )\n",
        "\n",
        "        # FeedForward block (pass config as necessary)\n",
        "        self.ff = FeedForward(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"drop_rate\"])\n",
        "\n",
        "        # LayerNorm for both attention and feed-forward\n",
        "        self.norm1 = LayerNorm(GPT_CONFIG_124M[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(GPT_CONFIG_124M[\"emb_dim\"])\n",
        "\n",
        "        # Dropout layers for residual connections\n",
        "        self.drop_shortcut = nn.Dropout(GPT_CONFIG_124M[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)  # LayerNorm first\n",
        "        x = self.att(x)  # Pass through attention layer\n",
        "        x = self.drop_shortcut(x)  # Apply dropout\n",
        "        x = x + shortcut  # Add original input (residual connection)\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)  # Apply LayerNorm\n",
        "        x = self.ff(x)  # Pass through feed-forward network\n",
        "        x = self.drop_shortcut(x)  # Apply dropout\n",
        "        x = x + shortcut  # Add residual connection\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P7HAq-Z-8yHT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from src.models.transformer_block import TransformerBlock\n",
        "# from src.models.layer_norm import LayerNorm\n",
        "# from src.models.config import GPT_CONFIG_124M  # Import the config directly\n",
        "\n",
        "\n",
        "class GPT2Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Access configuration directly from GPT_CONFIG_124M\n",
        "        config = GPT_CONFIG_124M  # Access configuration directly\n",
        "\n",
        "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "        # Use nn.Sequential to stack the transformer blocks as per the configuration\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock() for _ in range(config[\"n_layers\"])]  # Stack blocks sequentially\n",
        "        )\n",
        "\n",
        "        # Final LayerNorm and output projection layer\n",
        "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "\n",
        "        # Get token and position embeddings\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device).unsqueeze(0).expand(batch_size, -1))  # Corrected position embedding\n",
        "\n",
        "        # Add token and position embeddings together\n",
        "        x = tok_embeds + pos_embeds  # Shape: [batch_size, num_tokens, emb_size]\n",
        "\n",
        "        # Apply dropout on the embeddings\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        # Pass through all transformer blocks sequentially\n",
        "        x = self.trf_blocks(x)\n",
        "\n",
        "        # Apply the final LayerNorm\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Generate logits via the output head (projection layer)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDp0vMGiA6AU",
        "outputId": "4920e75b-8e90-445c-bdfc-cb17941a83ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eyRSO-0s9AAX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "from transformers import get_scheduler\n",
        "\n",
        "# Define the directory for saving the model\n",
        "model_path = \"/content/drive/MyDrive/gpt2/gpt2_model\"  # Desired path for saving the model\n",
        "os.makedirs(model_path, exist_ok=True)  # Create the directory and any necessary parent directories\n",
        "\n",
        "def save_model(model, path=None):\n",
        "    \"\"\"\n",
        "    Saves the trained GPT-2 model to the specified path.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The trained model.\n",
        "    - path (str): Path to save the model. Defaults to the `model_path` directory.\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        path = os.path.join(model_path, \"gpt2_model.pth\")  # Default path for saving the model\n",
        "\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def train_model(model, train_loader, config, device):\n",
        "    \"\"\"\n",
        "    Trains the GPT-2 model.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The GPT-2 model to be trained.\n",
        "    - train_loader (DataLoader): The DataLoader for the training dataset.\n",
        "    - config (dict): Training configuration including learning rate, weight decay, and epochs.\n",
        "    - device (torch.device): The device to use for training (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    - train_loss_history (list): A list of average loss values for each epoch.\n",
        "    - train_accuracy_history (list): A list of average accuracy values for each epoch.\n",
        "    - model (torch.nn.Module): The trained model.\n",
        "    \"\"\"\n",
        "    # Print the device being used\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Ensure the model is on the correct device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "    total_steps = len(train_loader) * config[\"epochs\"]\n",
        "    scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Loss function\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    train_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch {epoch+1}/{config['epochs']}\")\n",
        "        train_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        for input_ids, target_ids in tqdm(train_loader, desc=\"Training\"):\n",
        "            # Move data to the correct device (GPU or CPU)\n",
        "            input_ids = input_ids.to(device)\n",
        "            target_ids = target_ids.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted_ids = logits.argmax(dim=-1)  # Get the token with the highest probability\n",
        "            correct_predictions += (predicted_ids == target_ids).sum().item()\n",
        "            total_predictions += target_ids.numel()\n",
        "\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        avg_accuracy = correct_predictions / total_predictions * 100  # Percentage accuracy\n",
        "        train_loss_history.append(avg_loss)\n",
        "        train_accuracy_history.append(avg_accuracy)\n",
        "\n",
        "        print(f\"Training Loss: {avg_loss:.4f}, Training Accuracy: {avg_accuracy:.2f}%\")\n",
        "\n",
        "    # Save the model after the final epoch\n",
        "    save_model(model, path=os.path.join(model_path, \"gpt2_model_final.pth\"))\n",
        "\n",
        "    return train_loss_history, train_accuracy_history, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mg-uQgML9FTY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "def validate_model(model, val_loader, device):\n",
        "    \"\"\"\n",
        "    Validate the GPT-2 model on the validation set.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The GPT-2 model to be validated.\n",
        "    - val_loader (DataLoader): The DataLoader for the validation dataset.\n",
        "    - device (torch.device): The device to use for validation (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    - avg_loss (float): The average loss over the validation set.\n",
        "    - avg_accuracy (float): The average accuracy over the validation set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation needed during validation\n",
        "        for input_ids, target_ids in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            target_ids = target_ids.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted_ids = logits.argmax(dim=-1)  # Get the token with the highest probability\n",
        "            correct_predictions += (predicted_ids == target_ids).sum().item()\n",
        "            total_predictions += target_ids.numel()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    avg_accuracy = correct_predictions / total_predictions * 100  # Percentage accuracy\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}, Validation Accuracy: {avg_accuracy:.2f}%\")\n",
        "\n",
        "    return avg_loss, avg_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QdrwNhRA9HH4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_loss(train_loss_history, val_loss_history):\n",
        "    epochs = len(train_loss_history)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, epochs + 1), train_loss_history, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(range(1, epochs + 1), val_loss_history, label=\"Validation Loss\", marker='o')\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"/content/drive/MyDrive/gpt2/tests/loss_plot.png\")\n",
        "    print(f\"Loss plot saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mhk6Cv8l9MKH",
        "outputId": "2131c3d7-7f6c-4643-8918-0765d280faed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cuda\n",
            "Training batches: 81\n",
            "Validation batches: 9\n",
            "Initializing model...\n",
            "Starting training on cuda...\n",
            "Training on device: cuda\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:50<00:00,  1.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 6.1484, Training Accuracy: 26.83%\n",
            "Epoch 2/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:51<00:00,  1.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 4.7306, Training Accuracy: 32.58%\n",
            "Epoch 3/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 4.1267, Training Accuracy: 37.70%\n",
            "Epoch 4/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 3.7001, Training Accuracy: 41.00%\n",
            "Epoch 5/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 3.3805, Training Accuracy: 43.94%\n",
            "Epoch 6/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 3.0998, Training Accuracy: 46.65%\n",
            "Epoch 7/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.8770, Training Accuracy: 49.13%\n",
            "Epoch 8/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.6630, Training Accuracy: 51.89%\n",
            "Epoch 9/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.4754, Training Accuracy: 54.40%\n",
            "Epoch 10/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.3165, Training Accuracy: 56.90%\n",
            "Epoch 11/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.1584, Training Accuracy: 59.15%\n",
            "Epoch 12/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 2.0183, Training Accuracy: 61.44%\n",
            "Epoch 13/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.8872, Training Accuracy: 63.57%\n",
            "Epoch 14/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.7725, Training Accuracy: 65.65%\n",
            "Epoch 15/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.6718, Training Accuracy: 67.52%\n",
            "Epoch 16/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.5773, Training Accuracy: 69.19%\n",
            "Epoch 17/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.4906, Training Accuracy: 70.62%\n",
            "Epoch 18/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.4199, Training Accuracy: 72.09%\n",
            "Epoch 19/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.3534, Training Accuracy: 73.37%\n",
            "Epoch 20/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.2989, Training Accuracy: 74.47%\n",
            "Epoch 21/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.2535, Training Accuracy: 75.38%\n",
            "Epoch 22/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.2154, Training Accuracy: 76.22%\n",
            "Epoch 23/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.1820, Training Accuracy: 76.90%\n",
            "Epoch 24/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.1605, Training Accuracy: 77.41%\n",
            "Epoch 25/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 81/81 [00:52<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.1455, Training Accuracy: 77.73%\n",
            "Model saved to /content/drive/MyDrive/gpt2/gpt2_model/gpt2_model_final.pth\n",
            "Epoch 1/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 1: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 2/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 2: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 3/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 3: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 4/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 4: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 5/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 5: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 6/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 6: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 7/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 7: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 8/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 8: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 9/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 9: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 10/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 10: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 11/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 11: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 12/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 12: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 13/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 13: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 14/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 14: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 15/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 15: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 16/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 16: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 17/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 17: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 18/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 18: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 19/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 19: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 20/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 20: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 21/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 21: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 22/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 22: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 23/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 23: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 24/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 24: (3.368541955947876, 46.20768229166667)\n",
            "Epoch 25/25 - Validating...\n",
            "Validation Loss: 3.3685, Validation Accuracy: 46.21%\n",
            "Validation loss after epoch 25: (3.368541955947876, 46.20768229166667)\n",
            "Saving the trained model to gpt2_model.pth...\n",
            "Model saved to gpt2_model.pth\n",
            "Plotting the loss...\n",
            "Loss plot saved\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "# from src.models.gpt2_model import GPT2Model\n",
        "# from src.create_dataset.data_loading_splitting import load_and_split_data\n",
        "# from src.models.config import GPT_CONFIG_124M\n",
        "# from tests.train import train_model, save_model\n",
        "# from tests.evaluate import validate_model\n",
        "# from tests.visualisation import plot_loss\n",
        "\n",
        "def main(csv_file, model_save_path=\"gpt2_model.pth\", device=\"cuda\"):\n",
        "    # Load config\n",
        "    config = GPT_CONFIG_124M\n",
        "\n",
        "    # Load data\n",
        "    train_loader, val_loader = load_and_split_data(csv_file=csv_file, config=config)\n",
        "\n",
        "    # Check if data is loading correctly\n",
        "    print(f\"Training batches: {len(train_loader)}\")  # Check how many batches you have\n",
        "    print(f\"Validation batches: {len(val_loader)}\")  # Check how many validation batches you have\n",
        "\n",
        "    # Initialize model\n",
        "    print(f\"Initializing model...\")\n",
        "    model = GPT2Model().to(device)\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"Starting training on {device}...\")\n",
        "    # Unpack three values from train_model (train_loss_history, train_accuracy_history, and model)\n",
        "    train_loss_history, train_accuracy_history, model = train_model(model, train_loader, config, device)\n",
        "\n",
        "    # Initialize an empty list for validation loss history\n",
        "    val_loss_history = []\n",
        "\n",
        "    # Validate the model after each epoch\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        print(f\"Epoch {epoch + 1}/{config['epochs']} - Validating...\")\n",
        "        val_loss = validate_model(model, val_loader, device)\n",
        "        print(f\"Validation loss after epoch {epoch + 1}: {val_loss}\")\n",
        "        val_loss_history.append(val_loss)\n",
        "\n",
        "    # Save the trained model\n",
        "    print(f\"Saving the trained model to {model_save_path}...\")\n",
        "    save_model(model, model_save_path)\n",
        "\n",
        "    # Visualize training and validation loss\n",
        "    print(f\"Plotting the loss...\")\n",
        "    plot_loss(train_loss_history, val_loss_history)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file = r\"/content/drive/MyDrive/gpt2/data/raw/medquad_1000.csv\"  # Replace with the path to your dataset\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Running on device: {device}\")\n",
        "    main(csv_file, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In the plot top line green color is  validation accuracy not validation loss "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKzLKtyP-I13"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L1DS7uZPeOy",
        "outputId": "36f964ce-3372-4780-9203-6d4b45f7460b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on device: cuda\n",
            "Chatbot ready. Type 'exit' to end the chat.\n",
            "You: Kidney Disease in Children\n",
            "Bot: Kidney Disease in Children\n",
            "Answer at greater risk for risk becoming groups offer classes and heart disease with cirrhosis should include heart attacks will come in which is used as a higher-American men and saturated fat or having heart problems such meat. You can prevent severe heart problems\n",
            "You: Kidney Disease in Children\n",
            "Bot: Kidney Disease in Children with an estimate of these medical terms or the body, they arent drink or    External radiation is a light sed, or injected near a vein may receive an important from growing before being tested to make a microscope atypical hernia \n",
            "You: Kidney Disease in Children\n",
            "Bot: Kidney Disease in Children with the body's body to a body and a microscope by the blood cells called biotherapy:\n",
            "  Some researchers are able can help reduce these tests. This procedure involves drawing a light. The pediatric from another protein are actually help prevent an outpatient-\n",
            "You:  congenita inherited\n",
            "Bot:  congenita inherited and how often lead to be right away if there in one studyation.  Physical problems caused swelling, which can treat certain areas. When chemotherapy before the amount is not be different if they also take any remaining the body such between the production will look\n",
            "You:  symptoms of Liddle syndrome\n",
            "Bot:  symptoms of Liddle syndrome. Signs the lymphocyst below the symptom will also remove as patients were done. If people may also called nuclear Neurological insufficiency treatment or stop bleeding can live to be seen for long. In addition or after they take it only during treatment\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "# from src.models.gpt2_model import GPT2Model\n",
        "# from src.models.config import GPT_CONFIG_124M\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    \"\"\"\n",
        "    Load the model's weights and return the model.\n",
        "    \"\"\"\n",
        "    model = GPT2Model().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))  # Load only weights\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "def sample_next_token_nucleus(logits, p=0.9, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample the next token using nucleus (top-p) sampling with temperature scaling.\n",
        "    \"\"\"\n",
        "    # Apply temperature scaling to logits\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Clip extreme logits to prevent numerical overflow\n",
        "    logits = torch.clamp(logits, -10.0, 10.0)\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Check for NaN or Inf in probabilities and replace them with zero\n",
        "    if torch.any(torch.isnan(probs)) or torch.any(torch.isinf(probs)):\n",
        "        print(\"Warning: NaN or Inf found in probs\")\n",
        "        print(f\"Logits before softmax: {logits}\")\n",
        "        raise ValueError(\"NaN or Inf detected in the probability distribution.\")\n",
        "\n",
        "    # Sort probabilities in descending order and get cumulative probabilities\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    # Select only the tokens with cumulative probability less than p\n",
        "    sorted_indices_to_keep = cumulative_probs < p\n",
        "    sorted_probs[~sorted_indices_to_keep] = 0  # Set others to zero\n",
        "\n",
        "    # Add a small epsilon value to avoid division by zero during renormalization\n",
        "    epsilon = 1e-8\n",
        "    sorted_probs /= sorted_probs.sum() + epsilon  # Re-normalize to form a valid probability distribution\n",
        "\n",
        "    # Check for any NaN or Inf values after re-normalization\n",
        "    if torch.any(torch.isnan(sorted_probs)) or torch.any(torch.isinf(sorted_probs)):\n",
        "        print(\"Warning: NaN or Inf found after renormalization\")\n",
        "        print(f\"Sorted Probs: {sorted_probs}\")\n",
        "        raise ValueError(\"NaN or Inf detected after re-normalizing the probability distribution.\")\n",
        "\n",
        "    # Sample the next token from the filtered probabilities\n",
        "    next_token = torch.multinomial(sorted_probs, 1)\n",
        "\n",
        "    # Return the corresponding token index from sorted indices\n",
        "    return sorted_indices.gather(-1, next_token)\n",
        "\n",
        "def generate_response(model, prompt, tokenizer, max_length=50, device=\"cpu\", temperature=0.7, top_k=20, p=0.9):\n",
        "    input_tokens = tokenizer.encode(prompt, allowed_special={'<|endoftext|>'})\n",
        "    input_tensor = torch.tensor(input_tokens).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    generated_tokens = input_tensor\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get the model's output (logits)\n",
        "            logits = model(generated_tokens)\n",
        "\n",
        "            # Focus on the last token's logits\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Sample the next token using nucleus sampling (top-p)\n",
        "            next_token = sample_next_token_nucleus(logits, p=p, temperature=temperature)\n",
        "\n",
        "            # Append the new token to the context\n",
        "            generated_tokens = torch.cat((generated_tokens, next_token), dim=1)\n",
        "\n",
        "            # Optionally check for the end-of-text token\n",
        "            end_of_text_token = tokenizer.encode(\"<|endoftext|>\", allowed_special={'<|endoftext|>'})[0]\n",
        "            if next_token.item() == end_of_text_token:\n",
        "                break\n",
        "\n",
        "    # Decode the generated tokens to text\n",
        "    output_text = tokenizer.decode(generated_tokens.squeeze().cpu().tolist())\n",
        "\n",
        "    return output_text\n",
        "\n",
        "def chat(model, tokenizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Chat interface for interacting with the trained model.\n",
        "    \"\"\"\n",
        "    print(\"Chatbot ready. Type 'exit' to end the chat.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Generate response using the trained model\n",
        "        response = generate_response(model, user_input, tokenizer, device=device, temperature=0.7, top_k=50, p=0.9)\n",
        "        print(f\"Bot: {response}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to initialize and run the chatbot.\n",
        "    \"\"\"\n",
        "    model_path = r\"/content/drive/MyDrive/gpt2/gpt2_model/gpt2_model_final.pth\"  # Path to the trained model\n",
        "\n",
        "    # Automatically choose device based on availability\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Running on device: {device}\")\n",
        "\n",
        "    # Load the tokenizer and the trained model\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    model = load_model(model_path, device)\n",
        "\n",
        "    # Start the chatbot\n",
        "    chat(model, tokenizer, device=device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
